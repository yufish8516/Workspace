{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OYlaRwNu7ojq"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**\n",
        "\n",
        "* Slides: https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW02/HW02.pdf\n",
        "* Video (Chinese): https://youtu.be/PdjXnQbu2zo\n",
        "* Video (English): https://youtu.be/ESRr-VCykBs\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "emUd7uS7crTz"
      },
      "source": [
        "## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
        "\n",
        "This homework is a multiclass classification task,\n",
        "we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
        "\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KVUGfWTo7_Oj"
      },
      "source": [
        "## Download Data\n",
        "Download data from google drive, then unzip it.\n",
        "\n",
        "You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n",
        "`timit_11/`\n",
        "- `train_11.npy`: training data<br>\n",
        "- `train_label_11.npy`: training label<br>\n",
        "- `test_11.npy`:  testing data<br><br>\n",
        "\n",
        "**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkiMEcC3Foq",
        "outputId": "ff280510-b560-454e-e41e-b10e223788ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR\n",
            "To: /content/data.zip\n",
            "100% 372M/372M [00:02<00:00, 185MB/s]\n",
            "Archive:  data.zip\n",
            "   creating: timit_11/\n",
            "  inflating: timit_11/train_11.npy   \n",
            "  inflating: timit_11/test_11.npy    \n",
            "  inflating: timit_11/train_label_11.npy  \n",
            "data.zip  sample_data  timit_11\n"
          ]
        }
      ],
      "source": [
        "!gdown --id '1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_4anls8Drv"
      },
      "source": [
        "## Preparing Data\n",
        "Load the training and testing data from the `.npy` file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJjLT8em-y9G",
        "outputId": "fa5e07eb-cba1-4df1-89b1-4123ec95d989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Loading data ...')\n",
        "\n",
        "data_root='./timit_11/'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "print('Size of training data: {}'.format(train.shape))\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "us5XW_x6udZQ"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fjf5EcmJtf4e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "otIC6WhGeh9v"
      },
      "source": [
        "Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYqi_lAuvC59",
        "outputId": "cadf344f-a49b-434c-a879-520430d373c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of training set: (1217632, 429)\n",
            "Size of validation set: (12300, 429)\n"
          ]
        }
      ],
      "source": [
        "VAL_RATIO = 0.01\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
        "print('Size of training set: {}'.format(train_x.shape))\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nbCfclUIgMTX"
      },
      "source": [
        "Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUCbQvqJurYc",
        "outputId": "3a0a245a-e7d0-4d98-da94-e801b1daa251"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-f6af567917fe>:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = y.astype(np.int)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 2048\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train_x, train_y)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True) #only shuffle the training data\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_SY7X0lUgb50"
      },
      "source": [
        "Cleanup the unneeded variables to save memory.<br>\n",
        "\n",
        "**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8rzkGraeYeN",
        "outputId": "74fd73e0-cae2-4a84-e316-817387b7916d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, train_x, train_y, val_x, val_y\n",
        "gc.collect()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqKNvNZwe3V"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FYr1ng5fh9pA"
      },
      "source": [
        "Define model architecture, you are encouraged to change and experiment with the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lbZrwT6Ny0XL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(429, 2048),\n",
        "            nn.LeakyReLU(),\n",
        "            #nn.ReLU(),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.Dropout(0.45),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.LeakyReLU(),\n",
        "            #nn.ReLU(),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.Dropout(0.45),\n",
        "            nn.Linear(2048,1024),\n",
        "            nn.LeakyReLU(),\n",
        "            #nn.ReLU(),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(0.45),\n",
        "            nn.Linear(1024, 512),\n",
        "            #nn.ReLU(),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.45),\n",
        "            nn.Linear(512, 256), \n",
        "            #nn.ReLU(),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.45),\n",
        "            nn.Linear(256, 128),\n",
        "            #nn.ReLU(),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.45),\n",
        "            nn.Linear(128, 39)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VRYciXZvPbYh"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y114Vmm3Ja6o"
      },
      "outputs": [],
      "source": [
        "#check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sEX-yjHjhGuH"
      },
      "source": [
        "Fix random seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "88xPiUnm0tAd"
      },
      "outputs": [],
      "source": [
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KbBcBXkSp6RA"
      },
      "source": [
        "Feel free to change the training parameters here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTp3ZXg1yO9Y",
        "outputId": "f19096be-08f8-4ade-de50-d7754353a7bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ]
        }
      ],
      "source": [
        "# fix random seed for reproducibility\n",
        "same_seeds(0)\n",
        "\n",
        "# get device\n",
        "device = get_device()\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# training parameters\n",
        "num_epoch = 64           # number of training epoch\n",
        "learning_rate = 0.001       # learning rate\n",
        "\n",
        "# the path where checkpoint saved\n",
        "model_path = './model.ckpt'\n",
        "\n",
        "# create model, define a loss function, and optimizer\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.NAdam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdMWsBs7zzNs",
        "outputId": "542cd8c0-f4b1-44e2-9e70-496b827fcbac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[001/064] Train Acc: 0.551996 Loss: 1.544222 | Val Acc: 0.659756 loss: 1.015554\n",
            "saving model with acc 0.660\n",
            "[002/064] Train Acc: 0.633404 Loss: 1.225497 | Val Acc: 0.679187 loss: 0.914063\n",
            "saving model with acc 0.679\n",
            "[003/064] Train Acc: 0.659010 Loss: 1.133728 | Val Acc: 0.688618 loss: 0.890250\n",
            "saving model with acc 0.689\n",
            "[004/064] Train Acc: 0.674683 Loss: 1.075858 | Val Acc: 0.699675 loss: 0.849820\n",
            "saving model with acc 0.700\n",
            "[005/064] Train Acc: 0.686254 Loss: 1.032080 | Val Acc: 0.705122 loss: 0.839043\n",
            "saving model with acc 0.705\n",
            "[006/064] Train Acc: 0.695185 Loss: 0.999927 | Val Acc: 0.710244 loss: 0.818365\n",
            "saving model with acc 0.710\n",
            "[007/064] Train Acc: 0.702339 Loss: 0.972941 | Val Acc: 0.712358 loss: 0.816002\n",
            "saving model with acc 0.712\n",
            "[008/064] Train Acc: 0.708828 Loss: 0.948056 | Val Acc: 0.714228 loss: 0.791343\n",
            "saving model with acc 0.714\n",
            "[009/064] Train Acc: 0.714304 Loss: 0.929134 | Val Acc: 0.721220 loss: 0.780877\n",
            "saving model with acc 0.721\n",
            "[010/064] Train Acc: 0.718684 Loss: 0.912129 | Val Acc: 0.722114 loss: 0.771151\n",
            "saving model with acc 0.722\n",
            "[011/064] Train Acc: 0.723036 Loss: 0.895588 | Val Acc: 0.724797 loss: 0.771753\n",
            "saving model with acc 0.725\n",
            "[012/064] Train Acc: 0.726492 Loss: 0.882291 | Val Acc: 0.724634 loss: 0.770649\n",
            "[013/064] Train Acc: 0.729838 Loss: 0.869202 | Val Acc: 0.727236 loss: 0.761986\n",
            "saving model with acc 0.727\n",
            "[014/064] Train Acc: 0.732969 Loss: 0.858319 | Val Acc: 0.727561 loss: 0.772417\n",
            "saving model with acc 0.728\n",
            "[015/064] Train Acc: 0.736599 Loss: 0.846252 | Val Acc: 0.732927 loss: 0.745440\n",
            "saving model with acc 0.733\n",
            "[016/064] Train Acc: 0.738963 Loss: 0.836041 | Val Acc: 0.725528 loss: 0.751515\n",
            "[017/064] Train Acc: 0.742089 Loss: 0.826094 | Val Acc: 0.730407 loss: 0.749851\n",
            "[018/064] Train Acc: 0.744392 Loss: 0.817827 | Val Acc: 0.735366 loss: 0.743805\n",
            "saving model with acc 0.735\n",
            "[019/064] Train Acc: 0.746896 Loss: 0.808207 | Val Acc: 0.732358 loss: 0.751357\n",
            "[020/064] Train Acc: 0.747875 Loss: 0.801853 | Val Acc: 0.731707 loss: 0.754309\n",
            "[021/064] Train Acc: 0.750558 Loss: 0.793230 | Val Acc: 0.738130 loss: 0.735213\n",
            "saving model with acc 0.738\n",
            "[022/064] Train Acc: 0.752476 Loss: 0.786713 | Val Acc: 0.733740 loss: 0.749740\n",
            "[023/064] Train Acc: 0.754518 Loss: 0.779153 | Val Acc: 0.736341 loss: 0.757443\n",
            "[024/064] Train Acc: 0.756582 Loss: 0.772381 | Val Acc: 0.732439 loss: 0.751750\n",
            "[025/064] Train Acc: 0.757617 Loss: 0.767470 | Val Acc: 0.737073 loss: 0.735820\n",
            "[026/064] Train Acc: 0.759338 Loss: 0.762309 | Val Acc: 0.735610 loss: 0.747797\n",
            "[027/064] Train Acc: 0.761302 Loss: 0.754457 | Val Acc: 0.737398 loss: 0.738639\n",
            "[028/064] Train Acc: 0.762242 Loss: 0.750443 | Val Acc: 0.739268 loss: 0.735428\n",
            "saving model with acc 0.739\n",
            "[029/064] Train Acc: 0.763692 Loss: 0.745198 | Val Acc: 0.734309 loss: 0.738719\n",
            "[030/064] Train Acc: 0.765122 Loss: 0.740146 | Val Acc: 0.739593 loss: 0.734931\n",
            "saving model with acc 0.740\n",
            "[031/064] Train Acc: 0.767014 Loss: 0.734297 | Val Acc: 0.741138 loss: 0.729936\n",
            "saving model with acc 0.741\n",
            "[032/064] Train Acc: 0.767806 Loss: 0.729869 | Val Acc: 0.740000 loss: 0.735634\n",
            "[033/064] Train Acc: 0.769191 Loss: 0.726017 | Val Acc: 0.741382 loss: 0.730857\n",
            "saving model with acc 0.741\n",
            "[034/064] Train Acc: 0.770725 Loss: 0.721220 | Val Acc: 0.737561 loss: 0.748364\n",
            "[035/064] Train Acc: 0.771305 Loss: 0.717789 | Val Acc: 0.739106 loss: 0.749557\n",
            "[036/064] Train Acc: 0.772885 Loss: 0.713834 | Val Acc: 0.742439 loss: 0.726281\n",
            "saving model with acc 0.742\n",
            "[037/064] Train Acc: 0.773407 Loss: 0.710971 | Val Acc: 0.741301 loss: 0.745972\n",
            "[038/064] Train Acc: 0.774827 Loss: 0.705418 | Val Acc: 0.741545 loss: 0.743602\n",
            "[039/064] Train Acc: 0.775895 Loss: 0.702414 | Val Acc: 0.741707 loss: 0.746728\n",
            "[040/064] Train Acc: 0.776600 Loss: 0.700047 | Val Acc: 0.743415 loss: 0.737083\n",
            "saving model with acc 0.743\n",
            "[041/064] Train Acc: 0.777902 Loss: 0.695512 | Val Acc: 0.746016 loss: 0.736133\n",
            "saving model with acc 0.746\n",
            "[042/064] Train Acc: 0.778350 Loss: 0.692607 | Val Acc: 0.745935 loss: 0.732978\n",
            "[043/064] Train Acc: 0.779316 Loss: 0.689921 | Val Acc: 0.744878 loss: 0.739055\n",
            "[044/064] Train Acc: 0.780711 Loss: 0.685319 | Val Acc: 0.742846 loss: 0.740238\n",
            "[045/064] Train Acc: 0.781312 Loss: 0.682463 | Val Acc: 0.743252 loss: 0.734287\n",
            "[046/064] Train Acc: 0.782478 Loss: 0.679268 | Val Acc: 0.743902 loss: 0.741280\n",
            "[047/064] Train Acc: 0.782892 Loss: 0.676016 | Val Acc: 0.742683 loss: 0.738643\n",
            "[048/064] Train Acc: 0.783936 Loss: 0.673860 | Val Acc: 0.740163 loss: 0.739021\n",
            "[049/064] Train Acc: 0.785023 Loss: 0.671177 | Val Acc: 0.741789 loss: 0.744552\n",
            "[050/064] Train Acc: 0.785893 Loss: 0.668453 | Val Acc: 0.739350 loss: 0.746457\n",
            "[051/064] Train Acc: 0.786622 Loss: 0.666144 | Val Acc: 0.745610 loss: 0.740660\n",
            "[052/064] Train Acc: 0.786736 Loss: 0.663740 | Val Acc: 0.745691 loss: 0.744346\n",
            "[053/064] Train Acc: 0.787587 Loss: 0.661984 | Val Acc: 0.744797 loss: 0.741046\n",
            "[054/064] Train Acc: 0.788385 Loss: 0.658489 | Val Acc: 0.741626 loss: 0.740417\n",
            "[055/064] Train Acc: 0.788462 Loss: 0.657453 | Val Acc: 0.745854 loss: 0.748942\n",
            "[056/064] Train Acc: 0.789153 Loss: 0.655084 | Val Acc: 0.747480 loss: 0.745549\n",
            "saving model with acc 0.747\n",
            "[057/064] Train Acc: 0.789630 Loss: 0.652528 | Val Acc: 0.743984 loss: 0.749358\n",
            "[058/064] Train Acc: 0.791135 Loss: 0.648877 | Val Acc: 0.744959 loss: 0.749175\n",
            "[059/064] Train Acc: 0.791891 Loss: 0.646442 | Val Acc: 0.744146 loss: 0.754061\n",
            "[060/064] Train Acc: 0.792468 Loss: 0.642908 | Val Acc: 0.749106 loss: 0.743461\n",
            "saving model with acc 0.749\n",
            "[061/064] Train Acc: 0.792395 Loss: 0.643337 | Val Acc: 0.744878 loss: 0.749605\n",
            "[062/064] Train Acc: 0.793482 Loss: 0.640985 | Val Acc: 0.742927 loss: 0.752585\n",
            "[063/064] Train Acc: 0.793491 Loss: 0.639163 | Val Acc: 0.746098 loss: 0.746508\n",
            "[064/064] Train Acc: 0.794582 Loss: 0.636976 | Val Acc: 0.743659 loss: 0.765289\n"
          ]
        }
      ],
      "source": [
        "# start training\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # validation\n",
        "    if len(val_set) > 0:\n",
        "        model.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                batch_loss = criterion(outputs, labels)\n",
        "                _, val_pred = torch.max(outputs, 1)\n",
        "\n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "\n",
        "            # if the model improves, save a checkpoint at this epoch\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "    else:\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "# if not validating, save the last epoch\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('saving model at last epoch')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hi7jTn3PX-m"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NfUECMFCn5VG"
      },
      "source": [
        "Create a testing dataset, and load model from the saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PKjtAScPWtr",
        "outputId": "65552a37-2eae-4314-89a6-8856ac5f3b64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create testing dataset\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# create model and load weights from checkpoint\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "940TtCCdoYd0"
      },
      "source": [
        "Make prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "84HU5GGjPqR0"
      },
      "outputs": [],
      "source": [
        "predict = []\n",
        "model.eval() # set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AWDf_C-omElb"
      },
      "source": [
        "Write prediction to a CSV file.\n",
        "\n",
        "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GuljYSPHcZir"
      },
      "outputs": [],
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Score\n",
        "\n",
        "## Private Score: 0.75176\n",
        "## Public Score: 0.75246\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
